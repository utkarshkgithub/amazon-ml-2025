# ğŸ›’ Amazon ML Challenge 2025: From Data to Top 15% in 3 Days  
### A Deep Dive into Multi-Modal Product Price Prediction

Welcome to the repository for my **Top 24% (Rank 1,267 / 7000+)** solution for the **Amazon ML Challenge 2025**.  
In just **3 days**, this project evolved from a simple baseline into a **multi-modal fusion pipeline** that combines product images and text descriptions to predict prices accurately.

---

## ğŸš€ The Journey: 3-Day Sprint to 50.31 SMAPE

Every ML competition is a story of iteration.  
We started simple and improved step-by-step â€” validating every choice.

| Stage | Technique Added | Features | Validation SMAPE | Î” Improvement |
|:------|:----------------|:----------:|:----------------:|:-------------:|
| Baseline | Simple CSV Features | ~1,000 | 51.82 | â€” |
| Stage 1 | + Multi-Model Image Embeddings | 5,159 | 51.20 | â†“ 0.62 |
| Stage 2 | + Advanced TF-IDF Features | 5,309 | 50.61 | â†“ 0.59 |
| Stage 3 | + Optuna Hyperparameter Tuning | 5,312 | 50.31 | â†“ 0.30 |
| Stage 4 | âˆ’ Removed Low-Impact Features | 5,306 | 50.31 | âœ“ No change |
| Stage 5 | Experimental (Rejected) | 5,359 | 50.48 | â†‘ 0.17 |

ğŸ“Š **Total Improvement:** 51.82 â†’ 50.31 SMAPE (âˆ’1.51)

---

## ğŸ—“ï¸ Competition Timeline

| Date | Highlights |
|------|-------------|
| **Oct 10** | ğŸ Competition Kick-off & Initial EDA |
| **Oct 11** | ğŸ’¡ Built baseline (51.82 SMAPE) |
| **Oct 12** | ğŸ–¼ï¸ Added 4 image embedding models â†’ 50.61 |
| **Oct 13** | âš™ï¸ Optuna tuning â†’ 50.31 and ğŸ† Final submission & ensemble confirmed |
---

## ğŸ§  Solution Architecture: Multi-Modal Fusion Pipeline
```ruby
ğŸ“¦ Input Data (Images, Text, Metadata)
â”£â”â” ğŸ–¼ï¸ Image Pipeline
â”ƒ â”£â€¢ Pretrained Models: ResNet50, EfficientNetB0, ViT, DenseNet121
â”ƒ â”—â€¢ Output â†’ 5,109-dim embeddings
â”£â”â” ğŸ“ Text Pipeline
â”ƒ â”£â€¢ TF-IDF (150 feats)
â”ƒ â”£â€¢ Word/Char counts (6 feats)
â”ƒ â”£â€¢ Engineered NLP feats (47 feats)
â”ƒ â”—â€¢ Output â†’ 197-dim text/meta feats
â”—â”â” ğŸ§  Fusion & Modeling
â”£â€¢ Merge â†’ 5,306 features
â”£â€¢ Gradient Boosting Core â†’ LightGBM
â”£â€¢ Ensemble â†’ 70% LGBM + 30% XGBoost
â”—â€¢ ğŸ’° Final Price Predictions
```
---

## ğŸ’¡ Key Learnings

- **Quality > Quantity** â†’ 5,306 well-selected features beat 5,359 noisy ones.  
- **Strong validation = confidence** â†’ our local CV matched leaderboard.  
- **Iterate wisely** â†’ start simple, measure, then scale.  
- **Smart ensembling** â†’ 70/30 LGBM-XGB worked better than 50/50.

---

## ğŸ”§ Technology Stack

| Category | Tools |
|-----------|-------|
| Data Handling | Python 3.10, NumPy, Pandas |
| ML Modeling | LightGBM 3.3.5, XGBoost 1.7.0, scikit-learn |
| Deep Learning | PyTorch, Hugging Face Transformers, Torchvision |
| Tuning | Optuna |
| NLP | NLTK |

---

## ğŸ§© Feature Engineering Highlights

### ğŸ–¼ï¸ Multi-Model Image Embeddings  
Combined **ResNet**, **EfficientNet**, **ViT**, and **DenseNet** for diverse visual signals.

### ğŸ“ Text & Metadata Features  
- **Premium Keywords:** detect â€œluxuryâ€, â€œprofessionalâ€, etc.  
- **Budget Keywords:** detect â€œcheapâ€, â€œbasicâ€, etc.  
- **Quantity Extraction:** regex-based â€œ500gâ€, â€œ2 pcsâ€ â†’ normalized numeric feature.  

---

## ğŸ“Š Model Performance Deep Dive

| Model | Features | Train SMAPE | Val SMAPE | Gap |
|:------|:----------:|:-------------:|:------------:|:------:|
| LightGBM | 5,306 | 12.05 | **50.31 âœ“** | 38.26 |
| XGBoost | 5,359 | 5.70 | 50.50 | 44.80 |

âœ… **Chosen:** LightGBM â†’ better generalization.

---

## ğŸ” Top 10 Important Features

| Source | Feature | Importance |
|:--------|:------------------------------|:-----------:|
| ğŸ–¼ï¸ Image | Image_ResNet_feature_1024 | 0.0234 |
| ğŸ–¼ï¸ Image | Image_EfficientNet_feat_512 | 0.0198 |
| ğŸ“ Text | TF-IDF_luxury | 0.0156 |
| ğŸ“ Text | Text_char_count | 0.0142 |
| ğŸ“Š Metadata | Category_electronics | 0.0128 |
| ğŸ–¼ï¸ Image | Image_ViT_feature_384 | 0.0115 |
| ğŸ“ Text | Premium_score | 0.0109 |
| âš–ï¸ Engineered | Quantity_value | 0.0098 |
| ğŸ“ Text | TF-IDF_brand | 0.0087 |
| ğŸ–¼ï¸ Image | Image_DenseNet_feat_768 | 0.0081 |

---

## ğŸ”® Future Improvements

### Short-Term (~49.5 SMAPE)
- [ ] Replace TF-IDF with **DeBERTa / Sentence-Transformer** embeddings.  
- [ ] Category-wise specialized models.  
- [ ] Meta-model stacking ensemble.

### Long-Term (~48.5 SMAPE)
- [ ] **Vision-Language models (CLIP)** for joint embeddings.  
- [ ] **External data**: competitor pricing, brand info, etc.

---

## âš™ï¸ Setup & Usage

### Prerequisites
- Python 3.8+
- CUDA 11.8+
- 16GB+ RAM, 60GB+ Disk

### ğŸ§­ Quick Start

```bash
# 1ï¸âƒ£ Clone
git clone https://github.com/yourusername/amazon-ml-2025.git
cd amazon-ml-2025

# 2ï¸âƒ£ Setup environment
python -m venv venv
source venv/bin/activate  # (Windows: venv\Scripts\activate)

# 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt
ğŸƒ Run the Pipeline
bash
Copy code
# Image features (~4h)
python src/features/image_embeddings.py --data_dir data/raw --output_dir data/processed

# Text features (~30m)
python src/features/text_features.py --train_file data/train.csv --output_dir data/processed

# Train model (~10m)
python src/models/train_lgb.py --feature_dir data/processed --output models/lgb_model.pkl

# Generate predictions
python inference.py \
  --model models/lgb_optimized.pkl \
  --test_features data/processed/test_features_final.npy \
  --output test_predictions.csv
<details> <summary>ğŸ“ <strong>Project Structure</strong></summary>
css
Copy code
</details>
ğŸ“š References
Papers

DeBERTa: Decoding-enhanced BERT with Disentangled Attention

EfficientNet: Rethinking Model Scaling

Vision Transformer (ViT)

Libraries

LightGBM Docs

Hugging Face Transformers

ğŸ¤ Contributing & Contact
Contributions welcome!

Author: Dhanu Gupta,Utkarsh Kumar,Somil Gupta, Ishant Singh [Team: PENTAGON]

GitHub: @Dhanugupta0

LinkedIn: [dhanugupta0](https://www.linkedin.com/in/dhanugupta0/)
```
<div align="center">
Made with â¤ï¸ and lots of â˜•

â­ If you found this project insightful, consider giving it a star! â­

</div>
