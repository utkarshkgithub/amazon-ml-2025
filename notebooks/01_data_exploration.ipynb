{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e6eafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current features: 5109\n",
      "Text stats features: (75000, 6)\n",
      "Advanced text features: (75000, 28)\n",
      "Category features: (75000, 13)\n",
      "\n",
      "Text stats columns: ['item_name_length', 'item_name_word_count', 'item_name_has_numbers', 'has_bullets', 'bullet_length', 'bullet_word_count']\n",
      "Advanced features columns: ['premium_score', 'premium_count', 'has_premium', 'budget_score', 'budget_count', 'has_budget', 'net_premium_score', 'material_score', 'material_count', 'has_premium_material', 'brand_score', 'has_known_brand', 'is_luxury_brand', 'unit_price_score', 'is_expensive_unit', 'text_length', 'word_count', 'item_name_length', 'has_bullet_points', 'has_organic', 'has_natural', 'has_pack', 'has_bulk', 'has_frozen', 'has_fresh', 'is_tea_related', 'is_flower_related', 'is_food_related']\n",
      "Category columns: ['is_tea_beverage', 'score_tea_beverage', 'is_food_snacks', 'score_food_snacks', 'is_organic_health', 'score_organic_health', 'is_bulk_products', 'score_bulk_products', 'is_gourmet_premium', 'score_gourmet_premium', 'category_count', 'has_any_category', 'avg_category_price']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load existing features\n",
    "X_train_clean = np.load('../data/processed/train_features_clean.npy')\n",
    "X_test_final = np.load('../data/processed/test_features_final.npy')\n",
    "\n",
    "print(f\"Current features: {X_train_clean.shape[1]}\")\n",
    "\n",
    "# Load CSV features (check if these are already in your features)\n",
    "train_text_stats = pd.read_csv('../data/processed/train_text_stats.csv')\n",
    "test_text_stats = pd.read_csv('../data/processed/test_text_stats.csv')\n",
    "\n",
    "train_advanced = pd.read_csv('../data/processed/train_advanced_text_features.csv')\n",
    "test_advanced = pd.read_csv('../data/processed/test_advanced_text_features.csv')\n",
    "\n",
    "print(f\"Text stats features: {train_text_stats.shape}\")\n",
    "print(f\"Advanced text features: {train_advanced.shape}\")\n",
    "\n",
    "# Check if already included by comparing shapes\n",
    "# If train_features_clean has 5109 features, CSV features might be missing\n",
    "\n",
    "# Load category features\n",
    "train_cat = pd.read_csv('../data/processed/train_category_features.csv')\n",
    "test_cat = pd.read_csv('../data/processed/test_category_features.csv')\n",
    "\n",
    "print(f\"Category features: {train_cat.shape}\")\n",
    "\n",
    "# Check columns\n",
    "print(\"\\nText stats columns:\", train_text_stats.columns.tolist())\n",
    "print(\"Advanced features columns:\", train_advanced.columns.tolist())\n",
    "print(\"Category columns:\", train_cat.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dba15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING FEATURES\n",
      "============================================================\n",
      "âœ“ Current features: (74758, 5109)\n",
      "âœ“ Text stats: (75000, 6)\n",
      "âœ“ Advanced text: (75000, 28)\n",
      "âœ“ Category: (75000, 13)\n",
      "\n",
      "============================================================\n",
      "APPLYING OUTLIER MASK\n",
      "============================================================\n",
      "Outliers to remove: 242 / 75000\n",
      "âœ“ After mask - Text stats: (74758, 6)\n",
      "âœ“ After mask - Advanced: (74758, 28)\n",
      "âœ“ After mask - Category: (74758, 13)\n",
      "\n",
      "============================================================\n",
      "COMBINING FEATURES\n",
      "============================================================\n",
      "âœ“ Enhanced train features: (74758, 5156)\n",
      "âœ“ Enhanced test features: (75000, 5156)\n",
      "âœ“ Total features: 5156 (was 5,109, added 47)\n",
      "\n",
      "============================================================\n",
      "TRAINING LIGHTGBM WITH ENHANCED FEATURES\n",
      "============================================================\n",
      "Train: (59806, 5156), Val: (14952, 5156)\n",
      "\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: LOAD ALL FEATURES\n",
    "# ============================================================\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load existing features (5,109)\n",
    "X_train_current = np.load('../data/processed/train_features_clean.npy')\n",
    "X_test_current = np.load('../data/processed/test_features_final.npy')\n",
    "\n",
    "print(f\"âœ“ Current features: {X_train_current.shape}\")\n",
    "\n",
    "# Load CSV features\n",
    "train_text_stats = pd.read_csv('../data/processed/train_text_stats.csv')\n",
    "test_text_stats = pd.read_csv('../data/processed/test_text_stats.csv')\n",
    "\n",
    "train_advanced = pd.read_csv('../data/processed/train_advanced_text_features.csv')\n",
    "test_advanced = pd.read_csv('../data/processed/test_advanced_text_features.csv')\n",
    "\n",
    "train_cat = pd.read_csv('../data/processed/train_category_features.csv')\n",
    "test_cat = pd.read_csv('../data/processed/test_category_features.csv')\n",
    "\n",
    "print(f\"âœ“ Text stats: {train_text_stats.shape}\")\n",
    "print(f\"âœ“ Advanced text: {train_advanced.shape}\")\n",
    "print(f\"âœ“ Category: {train_cat.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: APPLY OUTLIER MASK TO CSV FEATURES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"APPLYING OUTLIER MASK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load original target to recreate outlier mask\n",
    "y_train_full = np.load('../data/processed/train_target_final.npy')\n",
    "y_log = np.log1p(y_train_full)\n",
    "\n",
    "Q1 = np.percentile(y_log, 25)\n",
    "Q3 = np.percentile(y_log, 75)\n",
    "IQR = Q3 - Q1\n",
    "outlier_mask = (y_log >= Q1 - 1.5*IQR) & (y_log <= Q3 + 1.5*IQR)\n",
    "\n",
    "print(f\"Outliers to remove: {(~outlier_mask).sum()} / {len(outlier_mask)}\")\n",
    "\n",
    "# Apply mask to CSV features\n",
    "train_text_stats_clean = train_text_stats.iloc[outlier_mask].values\n",
    "train_advanced_clean = train_advanced.iloc[outlier_mask].values\n",
    "train_cat_clean = train_cat.iloc[outlier_mask].values\n",
    "\n",
    "print(f\"âœ“ After mask - Text stats: {train_text_stats_clean.shape}\")\n",
    "print(f\"âœ“ After mask - Advanced: {train_advanced_clean.shape}\")\n",
    "print(f\"âœ“ After mask - Category: {train_cat_clean.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: COMBINE ALL FEATURES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMBINING FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Combine train features\n",
    "X_train_enhanced = np.hstack([\n",
    "    X_train_current,           # 5,109 features\n",
    "    train_text_stats_clean,    # 6 features\n",
    "    train_advanced_clean,      # 28 features\n",
    "    train_cat_clean            # 13 features\n",
    "])\n",
    "\n",
    "# Combine test features\n",
    "X_test_enhanced = np.hstack([\n",
    "    X_test_current,\n",
    "    test_text_stats.values,\n",
    "    test_advanced.values,\n",
    "    test_cat.values\n",
    "])\n",
    "\n",
    "print(f\"âœ“ Enhanced train features: {X_train_enhanced.shape}\")\n",
    "print(f\"âœ“ Enhanced test features: {X_test_enhanced.shape}\")\n",
    "print(f\"âœ“ Total features: {X_train_enhanced.shape[1]} (was 5,109, added 47)\")\n",
    "\n",
    "# Save enhanced features\n",
    "np.save('../data/processed/train_features_enhanced_v1.npy', X_train_enhanced)\n",
    "np.save('../data/processed/test_features_enhanced_v1.npy', X_test_enhanced)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: TRAIN LIGHTGBM WITH ENHANCED FEATURES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING LIGHTGBM WITH ENHANCED FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load target\n",
    "y_train_full = np.load('../data/processed/train_target_clean.npy')\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_enhanced, y_train_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# LightGBM parameters (adjusted for more features)\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 2000,\n",
    "    'max_depth': 7,              # Increased from 6\n",
    "    'num_leaves': 40,            # Increased from 31\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,     # Reduced from 0.8 (more features)\n",
    "    'reg_alpha': 0.8,            # Increased regularization\n",
    "    'reg_lambda': 3.0,           # Increased regularization\n",
    "    'min_child_samples': 25,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    eval_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 5: EVALUATE\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    y_true_orig = np.expm1(y_true)\n",
    "    y_pred_orig = np.expm1(y_pred)\n",
    "    numerator = np.abs(y_pred_orig - y_true_orig)\n",
    "    denominator = (np.abs(y_true_orig) + np.abs(y_pred_orig)) / 2\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "train_pred = lgb_model.predict(X_train)\n",
    "val_pred = lgb_model.predict(X_val)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "train_mae = mean_absolute_error(y_train, train_pred)\n",
    "val_mae = mean_absolute_error(y_val, val_pred)\n",
    "val_smape = calculate_smape(y_val, val_pred)\n",
    "\n",
    "print(f\"\\nðŸ“Š RESULTS WITH ENHANCED FEATURES (5,156 total)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Train MAE: {train_mae:.4f}\")\n",
    "print(f\"Val MAE: {val_mae:.4f}\")\n",
    "print(f\"Gap: {val_mae - train_mae:.4f}\")\n",
    "print(f\"\\nValidation SMAPE: {val_smape:.2f}\")\n",
    "print(f\"\\nðŸŽ¯ COMPARISON:\")\n",
    "print(f\"   Previous (5,109 features): 51.39 SMAPE\")\n",
    "print(f\"   Current (5,156 features):  {val_smape:.2f} SMAPE\")\n",
    "print(f\"   Improvement: {51.39 - val_smape:.2f} points\")\n",
    "\n",
    "if val_smape < 51.39:\n",
    "    print(f\"\\nâœ… IMPROVED! Making test predictions...\")\n",
    "    \n",
    "    # Make test predictions\n",
    "    test_pred_log = lgb_model.predict(X_test_enhanced)\n",
    "    test_pred_original = np.expm1(test_pred_log)\n",
    "    \n",
    "    # Save\n",
    "    np.save('lightgbm_enhanced_test_predictions.npy', test_pred_original)\n",
    "    lgb_model.booster_.save_model('lightgbm_enhanced_v1.txt')\n",
    "    \n",
    "    print(f\"âœ… Predictions saved: lightgbm_enhanced_test_predictions.npy\")\n",
    "    print(f\"âœ… Model saved: lightgbm_enhanced_v1.txt\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ TEST PREDICTIONS:\")\n",
    "    print(f\"   Samples: {len(test_pred_original):,}\")\n",
    "    print(f\"   Min: ${test_pred_original.min():.2f}\")\n",
    "    print(f\"   Max: ${test_pred_original.max():.2f}\")\n",
    "    print(f\"   Mean: ${test_pred_original.mean():.2f}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Expected leaderboard SMAPE: ~{val_smape:.2f}\")\n",
    "    print(f\"   (Improvement from 51.825 â†’ ~{val_smape:.2f})\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ No improvement. Consider adding TF-IDF features.\")\n",
    "\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27c8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2244cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329bf78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
